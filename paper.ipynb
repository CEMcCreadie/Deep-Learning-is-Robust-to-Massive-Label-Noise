{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MSIc37sVRd-G"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DljoANWURd-I"
      },
      "source": [
        "<h2>CNN-4 Model</h2>\n",
        "\n",
        "Tradeoff between getting High performing model and trying to keep it as 'natural' as possible to make analysis later easier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0hwEYon4Rd-J"
      },
      "outputs": [],
      "source": [
        "class CNN4(nn.Module):\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        self.num_conv = 64\n",
        "        self.conv_layers = nn.Sequential(\n",
        "         nn.Conv2d(1,self.num_conv, kernel_size=3, stride=1, padding=1, bias = False),\n",
        "         nn.MaxPool2d(2,2),\n",
        "         nn.Conv2d(self.num_conv,self.num_conv, kernel_size=3, stride=1, padding=1, bias = False),\n",
        "         nn.Conv2d(self.num_conv,self.num_conv, kernel_size=3, stride=1, padding=1, bias = False),\n",
        "         nn.MaxPool2d(2,2),\n",
        "         nn.Conv2d(self.num_conv,self.num_conv, kernel_size=3, stride=1, padding=1, bias = False),\n",
        "        )\n",
        "        # 28 * 28 (Image) * num_conv * (0.5^(number of max pools*n_dim))\n",
        "        self.final_image_size = 7 * 7 * self.num_conv \n",
        "        self.fc1 = nn.Linear(self.final_image_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(-1, self.final_image_size) \n",
        "        x = F.gelu(self.fc1(x))\n",
        "        x = F.gelu(self.fc2(x))\n",
        "        return x "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "oH1gbwMLRd-J"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "t = torchvision.transforms.Compose([\n",
        "                       torchvision.transforms.ToTensor(),\n",
        "                       torchvision.transforms.Normalize(mean=(0), std=(1))]\n",
        "                       ) \n",
        "train_data, validation_data = torch.utils.data.random_split(torchvision.datasets.MNIST('/data/mnist', download=True, train=True, transform=t),[50000,10000])\n",
        "#Can be used to speed up training \n",
        "# train_data_sub = torch.utils.data.Subset(train_data, range(0,128))\n",
        "train_loader = torch.utils.data.DataLoader( train_data, \n",
        "                batch_size=BATCH_SIZE, drop_last=True, shuffle=True)\n",
        "validation_loader = torch.utils.data.DataLoader( validation_data, \n",
        "                batch_size=BATCH_SIZE, drop_last=True, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader( torchvision.datasets.MNIST('/data/mnist', download=True, train=False, transform=t), \n",
        "                batch_size=BATCH_SIZE, drop_last=True, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZt-X0v3Rd-K"
      },
      "source": [
        "<h2>Training</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDsPCeeWTeKw",
        "outputId": "f19c9c1c-59f1-4162-f9ac-9d871609bf83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "NVIDIA GeForce GTX 1080\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.device_count())\n",
        "print(torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qlLjK2kQSmjn"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 1e-4\n",
        "device = torch.device(\"cuda:0\")\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def training_loop(model, n_epochs, data_loader):\n",
        "    optimizer = torch.optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
        "    training_loss = []\n",
        "    validation_loss = []\n",
        "    for epoch in range(n_epochs): \n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(data_loader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "    \n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "    \n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    \n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        training_loss.append(running_loss/len(data_loader))\n",
        "        running_loss = 0.0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(validation_loader, 0):\n",
        "                # get the inputs; data is a list of [inputs, labels]\n",
        "                inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        \n",
        "                outputs = model(inputs)\n",
        "                loss = loss_fn(outputs, labels)\n",
        "                # print statistics\n",
        "                running_loss += loss.item()\n",
        "        print(f\"Epoch: {epoch}, Validation Loss: {running_loss/len(validation_loader)}\")\n",
        "        validation_loss.append(running_loss/len(validation_loader))\n",
        "        \n",
        "    print('Finished Training')\n",
        "    return training_loss, validation_loss\n",
        "\n",
        "def model_eval(model):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "    return 100 * correct // total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4mF_Bg9Rd-L"
      },
      "source": [
        "<h2>Changing labels</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "SN6-YYUORd-L"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "# noise = 0.99 #Roughly 100 noisy labels for every one 'good label'\n",
        "def generate_noise(noise=0.99):\n",
        "  noise_labels = np.array(train_data.dataset.targets) # Torch Subset.dataset \n",
        "  for (i, label) in enumerate(noise_labels):\n",
        "    if(np.random.uniform() <= noise):\n",
        "      noise_labels[i] = np.round(np.random.uniform(0, 9.0))\n",
        "  \n",
        "  noise_labels = torch.from_numpy(noise_labels)\n",
        "  \n",
        "  train_data_noise = copy.deepcopy(train_data)\n",
        "  train_data_noise.dataset.targets = noise_labels\n",
        "  train_loader_noise = torch.utils.data.DataLoader( train_data_noise, \n",
        "                  batch_size=BATCH_SIZE, drop_last=True, shuffle=True)\n",
        "  \n",
        "  print(train_data.dataset.targets)\n",
        "  print(train_data_noise.dataset.targets)\n",
        "  return train_loader_noise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4xcUxg8T1DV"
      },
      "source": [
        "<h1>Experiment</h1>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "TRAINING_RUNS = 300"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "o6Ol_vAiT0bh",
        "outputId": "44edc1df-e1f4-4c21-87f2-9d6994ded2ea"
      },
      "source": [
        "model = CNN4().to(device)\n",
        "training_loss,validation_loss = training_loop(model, TRAINING_RUNS, train_loader) #Model is updated inplace\n",
        "model_eval(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "trace1 = go.Scatter(x=np.arange(TRAINING_RUNS), y=training_loss, name=\"Training Loss\")\n",
        "trace2 = go.Scatter(x=np.arange(TRAINING_RUNS), y=validation_loss, name=\"Validation Loss\")\n",
        "layout = go.Layout(title='Training Loss')\n",
        "fig = go.Figure(data=[trace1, trace2], layout=layout)\n",
        "fig.update_xaxes(title_text=\"Epochs\")\n",
        "fig.update_yaxes(title_text=\"Loss\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcSjGDMc24Xi"
      },
      "source": [
        "perhaps longer training time is the trick here ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZHWY-gXbxPL",
        "outputId": "7ae818d7-0279-4d3c-e9df-491c39f8337d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([5, 0, 4,  ..., 5, 6, 8])\n",
            "tensor([8, 7, 0,  ..., 2, 8, 2])\n",
            "Epoch: 0, Validation Loss: 2.3223689985580935\n",
            "Epoch: 1, Validation Loss: 2.3081502524706035\n",
            "Epoch: 2, Validation Loss: 2.3021531785145783\n",
            "Epoch: 3, Validation Loss: 2.2926894495120416\n",
            "Epoch: 4, Validation Loss: 2.305039957547799\n",
            "Epoch: 5, Validation Loss: 2.2899697698079624\n",
            "Epoch: 6, Validation Loss: 2.3077561389177275\n",
            "Epoch: 7, Validation Loss: 2.315843411745169\n",
            "Epoch: 8, Validation Loss: 2.3097303830660305\n",
            "Epoch: 9, Validation Loss: 2.31729732797696\n",
            "Epoch: 10, Validation Loss: 2.305281012486189\n",
            "Epoch: 11, Validation Loss: 2.336098175782424\n",
            "Epoch: 12, Validation Loss: 2.3395146826903024\n",
            "Epoch: 13, Validation Loss: 2.390246181915968\n",
            "Epoch: 14, Validation Loss: 2.366066818053906\n",
            "Epoch: 15, Validation Loss: 2.4053787619639664\n",
            "Epoch: 16, Validation Loss: 2.410941103330025\n",
            "Epoch: 17, Validation Loss: 2.4549994331139784\n",
            "Epoch: 18, Validation Loss: 2.465253538046128\n",
            "Epoch: 19, Validation Loss: 2.524981263356331\n",
            "Epoch: 20, Validation Loss: 2.541200113602174\n",
            "Epoch: 21, Validation Loss: 2.546500717218106\n",
            "Epoch: 22, Validation Loss: 2.5579349795977273\n",
            "Epoch: 23, Validation Loss: 2.602389673391978\n",
            "Epoch: 24, Validation Loss: 2.643250638857866\n",
            "Epoch: 25, Validation Loss: 2.664621656521773\n",
            "Epoch: 26, Validation Loss: 2.700241580223426\n",
            "Epoch: 27, Validation Loss: 2.759609858959149\n",
            "Epoch: 28, Validation Loss: 2.7572545554393377\n",
            "Epoch: 29, Validation Loss: 2.790826209844687\n",
            "Epoch: 30, Validation Loss: 2.833137963062678\n",
            "Epoch: 31, Validation Loss: 2.8626091931110773\n",
            "Epoch: 32, Validation Loss: 2.913117887882086\n",
            "Epoch: 33, Validation Loss: 2.9268758403949247\n",
            "Epoch: 34, Validation Loss: 2.9775330485441747\n",
            "Epoch: 35, Validation Loss: 3.0242533393395252\n",
            "Epoch: 36, Validation Loss: 3.0412578613330155\n",
            "Epoch: 37, Validation Loss: 3.048034533476218\n",
            "Epoch: 38, Validation Loss: 3.1286903734390554\n",
            "Epoch: 39, Validation Loss: 3.1697937700992975\n",
            "Epoch: 40, Validation Loss: 3.220017673877569\n",
            "Epoch: 41, Validation Loss: 3.2964639892944922\n",
            "Epoch: 42, Validation Loss: 3.279379276893078\n",
            "Epoch: 43, Validation Loss: 3.2989223163861494\n",
            "Epoch: 44, Validation Loss: 3.3500193640207634\n",
            "Epoch: 45, Validation Loss: 3.486097296843162\n",
            "Epoch: 46, Validation Loss: 3.431485224992801\n",
            "Epoch: 47, Validation Loss: 3.494798427208876\n",
            "Epoch: 48, Validation Loss: 3.529496725553121\n",
            "Epoch: 49, Validation Loss: 3.5983886879224043\n",
            "Epoch: 50, Validation Loss: 3.660318000958516\n",
            "Epoch: 51, Validation Loss: 3.745441589600001\n",
            "Epoch: 52, Validation Loss: 3.759047669477952\n",
            "Epoch: 53, Validation Loss: 3.7541909233117714\n",
            "Epoch: 54, Validation Loss: 3.8929200164782696\n",
            "Epoch: 55, Validation Loss: 3.866852958997091\n",
            "Epoch: 56, Validation Loss: 3.955282050065505\n",
            "Epoch: 57, Validation Loss: 4.034978033640446\n",
            "Epoch: 58, Validation Loss: 4.096499527112032\n",
            "Epoch: 59, Validation Loss: 4.169454375902812\n",
            "Epoch: 60, Validation Loss: 4.123909941086402\n",
            "Epoch: 61, Validation Loss: 4.235750037126052\n",
            "Epoch: 62, Validation Loss: 4.28243072445576\n",
            "Epoch: 63, Validation Loss: 4.321648282882495\n",
            "Epoch: 64, Validation Loss: 4.347155026900462\n",
            "Epoch: 65, Validation Loss: 4.383714510844304\n",
            "Epoch: 66, Validation Loss: 4.503178803584515\n",
            "Epoch: 67, Validation Loss: 4.499690451683143\n",
            "Epoch: 68, Validation Loss: 4.627106844614714\n",
            "Epoch: 69, Validation Loss: 4.554190455338894\n",
            "Epoch: 70, Validation Loss: 4.729653586943944\n",
            "Epoch: 71, Validation Loss: 4.720689325760572\n",
            "Epoch: 72, Validation Loss: 4.7453571527432175\n",
            "Epoch: 73, Validation Loss: 4.846385372754855\n",
            "Epoch: 74, Validation Loss: 4.870251365961173\n",
            "Epoch: 75, Validation Loss: 5.0171101337824116\n",
            "Epoch: 76, Validation Loss: 5.0200536617865925\n",
            "Epoch: 77, Validation Loss: 4.965353291768294\n",
            "Epoch: 78, Validation Loss: 5.059823135534923\n",
            "Epoch: 79, Validation Loss: 5.10464448424486\n",
            "Epoch: 80, Validation Loss: 5.15543210047942\n",
            "Epoch: 81, Validation Loss: 5.135887547181203\n",
            "Epoch: 82, Validation Loss: 5.250304607244638\n",
            "Epoch: 83, Validation Loss: 5.183643249365\n",
            "Epoch: 84, Validation Loss: 5.257710113739356\n",
            "Epoch: 85, Validation Loss: 5.400797115686612\n",
            "Epoch: 86, Validation Loss: 5.559823253215888\n",
            "Epoch: 87, Validation Loss: 5.418868793126864\n",
            "Epoch: 88, Validation Loss: 5.506199616652268\n",
            "Epoch: 89, Validation Loss: 5.476337331991929\n",
            "Epoch: 90, Validation Loss: 5.571504657085125\n",
            "Epoch: 91, Validation Loss: 5.62970984669832\n",
            "Epoch: 92, Validation Loss: 5.607642936400878\n",
            "Epoch: 93, Validation Loss: 5.658864930654183\n",
            "Epoch: 94, Validation Loss: 5.775349493210133\n",
            "Epoch: 95, Validation Loss: 5.775915786241874\n",
            "Epoch: 96, Validation Loss: 5.817910885199522\n",
            "Epoch: 97, Validation Loss: 5.818480092745561\n",
            "Epoch: 98, Validation Loss: 5.907771806686353\n",
            "Epoch: 99, Validation Loss: 5.845375865697861\n",
            "Epoch: 100, Validation Loss: 6.05259878665973\n",
            "Epoch: 101, Validation Loss: 5.865946562626423\n",
            "Epoch: 102, Validation Loss: 6.0969368402774515\n",
            "Epoch: 103, Validation Loss: 6.133262725976797\n",
            "Epoch: 104, Validation Loss: 6.003994579498585\n",
            "Epoch: 105, Validation Loss: 6.075191705654829\n",
            "Epoch: 106, Validation Loss: 6.188849287155347\n",
            "Epoch: 107, Validation Loss: 6.103896126533166\n",
            "Epoch: 108, Validation Loss: 6.348713887807651\n",
            "Epoch: 109, Validation Loss: 6.276162263674614\n",
            "Epoch: 110, Validation Loss: 6.32973961570324\n",
            "Epoch: 111, Validation Loss: 6.425998119207529\n",
            "Epoch: 112, Validation Loss: 6.268820516574077\n",
            "Epoch: 113, Validation Loss: 6.3651272142544775\n",
            "Epoch: 114, Validation Loss: 6.333312461773555\n",
            "Epoch: 115, Validation Loss: 6.462304705228561\n",
            "Epoch: 116, Validation Loss: 6.527719138524471\n",
            "Epoch: 117, Validation Loss: 6.560190190107394\n",
            "Epoch: 118, Validation Loss: 6.417364462063863\n",
            "Epoch: 119, Validation Loss: 6.63135841412422\n",
            "Epoch: 120, Validation Loss: 6.605537067621182\n",
            "Epoch: 121, Validation Loss: 6.588744009152437\n",
            "Epoch: 122, Validation Loss: 6.571453307683651\n",
            "Epoch: 123, Validation Loss: 6.605008592208226\n",
            "Epoch: 124, Validation Loss: 6.596012907150464\n",
            "Epoch: 125, Validation Loss: 6.8081157971651125\n",
            "Epoch: 126, Validation Loss: 6.7639699043371735\n",
            "Epoch: 127, Validation Loss: 6.71703639244422\n",
            "Epoch: 128, Validation Loss: 6.801312179137499\n",
            "Epoch: 129, Validation Loss: 6.781450904332674\n",
            "Epoch: 130, Validation Loss: 6.745900426155481\n",
            "Epoch: 131, Validation Loss: 6.8652213093562\n",
            "Epoch: 132, Validation Loss: 6.967794059178768\n",
            "Epoch: 133, Validation Loss: 6.938489136023399\n",
            "Epoch: 134, Validation Loss: 7.116062306440794\n",
            "Epoch: 135, Validation Loss: 7.0004023145406675\n",
            "Epoch: 136, Validation Loss: 6.961404221180158\n",
            "Epoch: 137, Validation Loss: 6.916156296546642\n",
            "Epoch: 138, Validation Loss: 7.050527833975279\n",
            "Epoch: 139, Validation Loss: 7.110554146460998\n",
            "Epoch: 140, Validation Loss: 7.281101679190611\n",
            "Epoch: 141, Validation Loss: 7.0928844549717045\n",
            "Epoch: 142, Validation Loss: 7.104252778566801\n",
            "Epoch: 143, Validation Loss: 6.950312240765645\n",
            "Epoch: 144, Validation Loss: 7.159550946492415\n",
            "Epoch: 145, Validation Loss: 7.304938997977819\n",
            "Epoch: 146, Validation Loss: 7.153588389739012\n",
            "Epoch: 147, Validation Loss: 7.337202527584174\n",
            "Epoch: 148, Validation Loss: 7.273969414906624\n",
            "Epoch: 149, Validation Loss: 7.333150489208026\n",
            "Epoch: 150, Validation Loss: 7.188023818609042\n",
            "Epoch: 151, Validation Loss: 7.4548547726411085\n",
            "Epoch: 152, Validation Loss: 7.364526780752035\n",
            "Epoch: 153, Validation Loss: 7.513754785060883\n",
            "Epoch: 154, Validation Loss: 7.471260128877102\n",
            "Epoch: 155, Validation Loss: 7.352606354615627\n",
            "Epoch: 156, Validation Loss: 7.577930271625519\n",
            "Epoch: 157, Validation Loss: 7.519692396506285\n",
            "Epoch: 158, Validation Loss: 7.422740811720873\n",
            "Epoch: 159, Validation Loss: 7.695952546902192\n",
            "Epoch: 160, Validation Loss: 7.701843934181409\n",
            "Epoch: 161, Validation Loss: 7.56989867412127\n",
            "Epoch: 162, Validation Loss: 7.654386622783465\n",
            "Epoch: 163, Validation Loss: 7.767080145004468\n",
            "Epoch: 164, Validation Loss: 7.61592343984506\n",
            "Epoch: 165, Validation Loss: 7.674739259939927\n",
            "Epoch: 166, Validation Loss: 7.582805032913502\n",
            "Epoch: 167, Validation Loss: 7.807735275763732\n",
            "Epoch: 168, Validation Loss: 7.6386761252696695\n",
            "Epoch: 169, Validation Loss: 7.705444111273839\n",
            "Epoch: 170, Validation Loss: 7.739956837434035\n",
            "Epoch: 171, Validation Loss: 7.989787604564276\n",
            "Epoch: 172, Validation Loss: 7.676230274713957\n",
            "Epoch: 173, Validation Loss: 7.9450653699728155\n",
            "Epoch: 174, Validation Loss: 7.958547231478569\n",
            "Epoch: 175, Validation Loss: 7.760643492906522\n",
            "Epoch: 176, Validation Loss: 8.092023788354336\n",
            "Epoch: 177, Validation Loss: 7.921635024058513\n",
            "Epoch: 178, Validation Loss: 8.17898366237298\n",
            "Epoch: 179, Validation Loss: 7.865346647225893\n",
            "Epoch: 180, Validation Loss: 8.052564897598364\n",
            "Epoch: 181, Validation Loss: 8.095771075823368\n",
            "Epoch: 182, Validation Loss: 8.075313861553486\n",
            "Epoch: 183, Validation Loss: 8.053583302558996\n",
            "Epoch: 184, Validation Loss: 7.8341483030563746\n",
            "Epoch: 185, Validation Loss: 7.98729991607177\n",
            "Epoch: 186, Validation Loss: 7.925249984631171\n",
            "Epoch: 187, Validation Loss: 8.122778134468275\n",
            "Epoch: 188, Validation Loss: 8.008446383170593\n",
            "Epoch: 189, Validation Loss: 8.036720726734552\n",
            "Epoch: 190, Validation Loss: 8.060755104590685\n",
            "Epoch: 191, Validation Loss: 7.99918055610779\n",
            "Epoch: 192, Validation Loss: 8.08870352537204\n",
            "Epoch: 193, Validation Loss: 8.242288728555044\n",
            "Epoch: 194, Validation Loss: 8.119619297675598\n",
            "Epoch: 195, Validation Loss: 8.293779117938799\n",
            "Epoch: 196, Validation Loss: 8.17067534648455\n",
            "Epoch: 197, Validation Loss: 8.316397381898684\n",
            "Epoch: 198, Validation Loss: 8.273826224681658\n",
            "Epoch: 199, Validation Loss: 8.370853929947584\n",
            "Epoch: 200, Validation Loss: 8.27264925149771\n",
            "Epoch: 201, Validation Loss: 8.164911297651438\n",
            "Epoch: 202, Validation Loss: 8.298115582038195\n",
            "Epoch: 203, Validation Loss: 8.416400586947416\n",
            "Epoch: 204, Validation Loss: 8.289295135400234\n",
            "Epoch: 205, Validation Loss: 8.343288354384594\n",
            "Epoch: 206, Validation Loss: 8.285720290281834\n",
            "Epoch: 207, Validation Loss: 8.555552216676565\n",
            "Epoch: 208, Validation Loss: 8.564595297361032\n",
            "Epoch: 209, Validation Loss: 8.437772772251032\n",
            "Epoch: 210, Validation Loss: 8.382888129124275\n",
            "Epoch: 211, Validation Loss: 8.635911808564114\n",
            "Epoch: 212, Validation Loss: 8.559944487535036\n",
            "Epoch: 213, Validation Loss: 8.625206416998154\n",
            "Epoch: 214, Validation Loss: 8.585760993835253\n",
            "Epoch: 215, Validation Loss: 8.65914656718572\n",
            "Epoch: 216, Validation Loss: 8.610070172028664\n",
            "Epoch: 217, Validation Loss: 8.477666364266323\n",
            "Epoch: 218, Validation Loss: 8.908282151589027\n",
            "Epoch: 219, Validation Loss: 8.739235434776697\n",
            "Epoch: 220, Validation Loss: 8.473067890375088\n",
            "Epoch: 221, Validation Loss: 8.600095913960384\n",
            "Epoch: 222, Validation Loss: 8.472866636056166\n",
            "Epoch: 223, Validation Loss: 8.76351838081311\n",
            "Epoch: 224, Validation Loss: 8.757421950499216\n",
            "Epoch: 225, Validation Loss: 8.753300870076204\n",
            "Epoch: 226, Validation Loss: 8.7779367963473\n",
            "Epoch: 227, Validation Loss: 8.80880830456049\n",
            "Epoch: 228, Validation Loss: 8.728752964582199\n",
            "Epoch: 229, Validation Loss: 8.734692150201553\n",
            "Epoch: 230, Validation Loss: 8.776779404053322\n",
            "Epoch: 231, Validation Loss: 8.565202680917887\n",
            "Epoch: 232, Validation Loss: 8.667667508125305\n",
            "Epoch: 233, Validation Loss: 8.836330468838032\n",
            "Epoch: 234, Validation Loss: 8.644345216262035\n",
            "Epoch: 235, Validation Loss: 8.676905165880154\n",
            "Epoch: 236, Validation Loss: 8.797804948611137\n",
            "Epoch: 237, Validation Loss: 8.751201828320822\n",
            "Epoch: 238, Validation Loss: 8.907087043309824\n",
            "Epoch: 239, Validation Loss: 8.734529770337618\n",
            "Epoch: 240, Validation Loss: 8.9017884181096\n",
            "Epoch: 241, Validation Loss: 8.822684080172808\n",
            "Epoch: 242, Validation Loss: 8.788531361482082\n",
            "Epoch: 243, Validation Loss: 8.79618890163226\n",
            "Epoch: 244, Validation Loss: 8.847159414719313\n",
            "Epoch: 245, Validation Loss: 8.802671366777176\n",
            "Epoch: 246, Validation Loss: 8.73129262832495\n",
            "Epoch: 247, Validation Loss: 9.128964546399239\n",
            "Epoch: 248, Validation Loss: 9.211522677005865\n",
            "Epoch: 249, Validation Loss: 8.897309937538244\n",
            "Epoch: 250, Validation Loss: 9.035847147305807\n",
            "Epoch: 251, Validation Loss: 9.202406623424627\n",
            "Epoch: 252, Validation Loss: 8.955375743217957\n",
            "Epoch: 253, Validation Loss: 9.082112102936476\n",
            "Epoch: 254, Validation Loss: 9.422531551275497\n",
            "Epoch: 255, Validation Loss: 9.040754093573643\n",
            "Epoch: 256, Validation Loss: 9.139663989727314\n",
            "Epoch: 257, Validation Loss: 9.190552855149294\n",
            "Epoch: 258, Validation Loss: 8.885963634038584\n",
            "Epoch: 259, Validation Loss: 9.120594382286072\n",
            "Epoch: 260, Validation Loss: 8.83820589689108\n",
            "Epoch: 261, Validation Loss: 9.008147516311743\n",
            "Epoch: 262, Validation Loss: 9.399410174443172\n",
            "Epoch: 263, Validation Loss: 9.13419718772937\n",
            "Epoch: 264, Validation Loss: 9.131805860079252\n",
            "Epoch: 265, Validation Loss: 9.254929554768097\n",
            "Epoch: 266, Validation Loss: 9.168210379588299\n",
            "Epoch: 267, Validation Loss: 9.25019343999716\n",
            "Epoch: 268, Validation Loss: 9.41279405202621\n",
            "Epoch: 269, Validation Loss: 9.38808406316317\n",
            "Epoch: 270, Validation Loss: 9.31684107811023\n",
            "Epoch: 271, Validation Loss: 9.314910346116775\n",
            "Epoch: 272, Validation Loss: 9.179785578678816\n",
            "Epoch: 273, Validation Loss: 9.45746810008318\n",
            "Epoch: 274, Validation Loss: 9.327082050152313\n",
            "Epoch: 275, Validation Loss: 9.331180022313045\n",
            "Epoch: 276, Validation Loss: 9.477327906168425\n",
            "Epoch: 277, Validation Loss: 9.279878457387289\n",
            "Epoch: 278, Validation Loss: 9.308079580465952\n",
            "Epoch: 279, Validation Loss: 9.326260404709059\n",
            "Epoch: 280, Validation Loss: 9.431367811484215\n",
            "Epoch: 281, Validation Loss: 9.422067321263826\n",
            "Epoch: 282, Validation Loss: 9.417780099770962\n",
            "Epoch: 283, Validation Loss: 9.483411527596987\n",
            "Epoch: 284, Validation Loss: 9.296585932756082\n",
            "Epoch: 285, Validation Loss: 9.417461971441904\n",
            "Epoch: 286, Validation Loss: 9.342581452467503\n",
            "Epoch: 287, Validation Loss: 9.493816705850454\n",
            "Epoch: 288, Validation Loss: 9.596191277870766\n",
            "Epoch: 289, Validation Loss: 9.501788364006924\n",
            "Epoch: 290, Validation Loss: 9.436898271242777\n",
            "Epoch: 291, Validation Loss: 9.541314633993002\n",
            "Epoch: 292, Validation Loss: 9.661966918370663\n",
            "Epoch: 293, Validation Loss: 9.526109446317722\n",
            "Epoch: 294, Validation Loss: 9.672206883247082\n",
            "Epoch: 295, Validation Loss: 9.478147514355488\n",
            "Epoch: 296, Validation Loss: 9.522224637178274\n",
            "Epoch: 297, Validation Loss: 9.66963584912129\n",
            "Epoch: 298, Validation Loss: 9.645353139975132\n",
            "Epoch: 299, Validation Loss: 9.797137515667158\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 10 %\n"
          ]
        }
      ],
      "source": [
        "#noise = [0.1, 0.5, 0.99]\n",
        "noise = [0.99]\n",
        "accuracy = []\n",
        "noise_training_loss_arr = []\n",
        "noise_validation_loss_arr = []\n",
        "for n in noise:\n",
        "  model_noise = CNN4().to(device)\n",
        "  noise_training_loss, noise_validation_loss = training_loop(model_noise, TRAINING_RUNS, generate_noise(n))\n",
        "  noise_training_loss_arr.append(noise_training_loss)\n",
        "  noise_validation_loss_arr.append(noise_validation_loss)\n",
        "  accuracy.append(model_eval(model_noise))\n",
        "  torch.save(model_noise, f\"BIGmodelNoise{n}.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFLoCj-r3q9c",
        "outputId": "14929fa7-d3d6-4dd2-e94c-0ee5460c76b8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[10]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.plotly.v1+json": {
              "config": {
                "plotlyServerURL": "https://plot.ly"
              },
              "data": [
                {
                  "line": {
                    "color": "red"
                  },
                  "name": "Training Loss",
                  "type": "scatter",
                  "x": [
                    0,
                    1,
                    2,
                    3,
                    4,
                    5,
                    6,
                    7,
                    8,
                    9,
                    10,
                    11,
                    12,
                    13,
                    14,
                    15,
                    16,
                    17,
                    18,
                    19,
                    20,
                    21,
                    22,
                    23,
                    24,
                    25,
                    26,
                    27,
                    28,
                    29,
                    30,
                    31,
                    32,
                    33,
                    34,
                    35,
                    36,
                    37,
                    38,
                    39,
                    40,
                    41,
                    42,
                    43,
                    44,
                    45,
                    46,
                    47,
                    48,
                    49,
                    50,
                    51,
                    52,
                    53,
                    54,
                    55,
                    56,
                    57,
                    58,
                    59,
                    60,
                    61,
                    62,
                    63,
                    64,
                    65,
                    66,
                    67,
                    68,
                    69,
                    70,
                    71,
                    72,
                    73,
                    74,
                    75,
                    76,
                    77,
                    78,
                    79,
                    80,
                    81,
                    82,
                    83,
                    84,
                    85,
                    86,
                    87,
                    88,
                    89,
                    90,
                    91,
                    92,
                    93,
                    94,
                    95,
                    96,
                    97,
                    98,
                    99,
                    100,
                    101,
                    102,
                    103,
                    104,
                    105,
                    106,
                    107,
                    108,
                    109,
                    110,
                    111,
                    112,
                    113,
                    114,
                    115,
                    116,
                    117,
                    118,
                    119,
                    120,
                    121,
                    122,
                    123,
                    124,
                    125,
                    126,
                    127,
                    128,
                    129,
                    130,
                    131,
                    132,
                    133,
                    134,
                    135,
                    136,
                    137,
                    138,
                    139,
                    140,
                    141,
                    142,
                    143,
                    144,
                    145,
                    146,
                    147,
                    148,
                    149,
                    150,
                    151,
                    152,
                    153,
                    154,
                    155,
                    156,
                    157,
                    158,
                    159,
                    160,
                    161,
                    162,
                    163,
                    164,
                    165,
                    166,
                    167,
                    168,
                    169,
                    170,
                    171,
                    172,
                    173,
                    174,
                    175,
                    176,
                    177,
                    178,
                    179,
                    180,
                    181,
                    182,
                    183,
                    184,
                    185,
                    186,
                    187,
                    188,
                    189,
                    190,
                    191,
                    192,
                    193,
                    194,
                    195,
                    196,
                    197,
                    198,
                    199,
                    200,
                    201,
                    202,
                    203,
                    204,
                    205,
                    206,
                    207,
                    208,
                    209,
                    210,
                    211,
                    212,
                    213,
                    214,
                    215,
                    216,
                    217,
                    218,
                    219,
                    220,
                    221,
                    222,
                    223,
                    224,
                    225,
                    226,
                    227,
                    228,
                    229,
                    230,
                    231,
                    232,
                    233,
                    234,
                    235,
                    236,
                    237,
                    238,
                    239,
                    240,
                    241,
                    242,
                    243,
                    244,
                    245,
                    246,
                    247,
                    248,
                    249,
                    250,
                    251,
                    252,
                    253,
                    254,
                    255,
                    256,
                    257,
                    258,
                    259,
                    260,
                    261,
                    262,
                    263,
                    264,
                    265,
                    266,
                    267,
                    268,
                    269,
                    270,
                    271,
                    272,
                    273,
                    274,
                    275,
                    276,
                    277,
                    278,
                    279,
                    280,
                    281,
                    282,
                    283,
                    284,
                    285,
                    286,
                    287,
                    288,
                    289,
                    290,
                    291,
                    292,
                    293,
                    294,
                    295,
                    296,
                    297,
                    298,
                    299
                  ],
                  "xaxis": "x",
                  "y": [
                    2.278299414546786,
                    2.274310394530107,
                    2.272968653374842,
                    2.271324990653503,
                    2.2689217313761594,
                    2.2651682612837964,
                    2.2588587715439545,
                    2.248902138918829,
                    2.2355835717099883,
                    2.219394501971222,
                    2.1988480159781503,
                    2.175939989685249,
                    2.1511771356097866,
                    2.1247614265709314,
                    2.0967139830662562,
                    2.0681959938606136,
                    2.0387428043441185,
                    2.0090089349526905,
                    1.9784641072020488,
                    1.9477883215597742,
                    1.9182769665339539,
                    1.8873786856056634,
                    1.856396073568493,
                    1.8246048424827,
                    1.7944958645006148,
                    1.7637456810428604,
                    1.7339402380734492,
                    1.705242719448788,
                    1.6763237535419293,
                    1.6479673464228035,
                    1.6200491729527522,
                    1.5896486630818298,
                    1.5635165691223096,
                    1.5374152517272934,
                    1.5090483031718587,
                    1.4860939948262692,
                    1.4580877511434152,
                    1.4327023663914615,
                    1.4091353043627648,
                    1.383985230582319,
                    1.3626424217269912,
                    1.3363944255817257,
                    1.3160251966282897,
                    1.2930869709285364,
                    1.2724719531404836,
                    1.2526616436571225,
                    1.2332790916768903,
                    1.2134383331492982,
                    1.1969469944814104,
                    1.1769458675201356,
                    1.1570266228975317,
                    1.142024424752261,
                    1.1251831142835214,
                    1.1067326144814948,
                    1.0914843494547162,
                    1.0755524841374533,
                    1.0636618948623862,
                    1.0492479685746448,
                    1.0395879041255665,
                    1.0203928421996773,
                    1.0125987224862762,
                    0.9976798421613844,
                    0.984157479679386,
                    0.9775714725828354,
                    0.9638710895245572,
                    0.9568676298505671,
                    0.9442916133759421,
                    0.938281383255685,
                    0.9238918181723424,
                    0.9198749334803006,
                    0.9079037433091871,
                    0.9060957735921906,
                    0.8966425653742615,
                    0.8892806941087664,
                    0.8764208711865006,
                    0.8737466542310202,
                    0.8681198387727542,
                    0.8653120114330903,
                    0.8500812322511868,
                    0.85542379731787,
                    0.8386405373275967,
                    0.8405499058404744,
                    0.8294734088191226,
                    0.8272508058634976,
                    0.828827121498948,
                    0.819158254584796,
                    0.8169371118428002,
                    0.8126909030498829,
                    0.8009333777099504,
                    0.8067549646530353,
                    0.7961007349679625,
                    0.797178992216932,
                    0.791794309037214,
                    0.7945953575696286,
                    0.7867205857505566,
                    0.7802917828423266,
                    0.7831786005506137,
                    0.7721449155432009,
                    0.7809782541153067,
                    0.7680140862601514,
                    0.7611217633099623,
                    0.7740987304898597,
                    0.7649714213899705,
                    0.7606827844215721,
                    0.7591547229531175,
                    0.7499486907896861,
                    0.7547593154537846,
                    0.7551477755898092,
                    0.7435420039902881,
                    0.7552056789360034,
                    0.7393185883979868,
                    0.746833370938878,
                    0.7425484508323684,
                    0.7383999762324457,
                    0.7374914904505434,
                    0.7319149362655516,
                    0.7368111967296362,
                    0.7302879217139203,
                    0.7332994139606608,
                    0.7299773299949728,
                    0.7298416568016426,
                    0.7225167453193634,
                    0.7334052687570471,
                    0.717612159427699,
                    0.723860320022208,
                    0.7199284959436608,
                    0.7150985210389853,
                    0.713520594909501,
                    0.7165730952090856,
                    0.7183114051360937,
                    0.7125987324687186,
                    0.7067667295090215,
                    0.7116447543002732,
                    0.7160214932394546,
                    0.7019445081831703,
                    0.7074837102417604,
                    0.7107711315689258,
                    0.7069782650768375,
                    0.6990645145770835,
                    0.6973691789120932,
                    0.7040954394583208,
                    0.6933735611630311,
                    0.702807151978399,
                    0.6974643068948567,
                    0.6902933233964916,
                    0.690416974968321,
                    0.6994139345770609,
                    0.6913392363729077,
                    0.6897111960928816,
                    0.6872519828407774,
                    0.698456160724163,
                    0.6948925170441672,
                    0.6798854663815449,
                    0.6953831125961834,
                    0.6863429531569517,
                    0.6778199441358924,
                    0.6894174551784458,
                    0.6910897859595191,
                    0.6760591949835878,
                    0.6826301488486356,
                    0.6765452766464248,
                    0.6923082897084242,
                    0.6773289153483552,
                    0.677532700089578,
                    0.6777772459221787,
                    0.6830992463418066,
                    0.673098722141279,
                    0.6736301836938086,
                    0.6696828190909839,
                    0.6789390316254503,
                    0.6676965249981374,
                    0.672348093112666,
                    0.669069967982711,
                    0.6744227568200395,
                    0.6635130324798055,
                    0.6714752292434629,
                    0.6656274873949349,
                    0.6729792923488858,
                    0.662288459664194,
                    0.6756416843314482,
                    0.6601188780976014,
                    0.6640041157526946,
                    0.6719681834861655,
                    0.6616826772823377,
                    0.6594945917456922,
                    0.6676121668725558,
                    0.6595540627093077,
                    0.6660051348257843,
                    0.6602074885156564,
                    0.6626958717400072,
                    0.6629555468255518,
                    0.6535666095672436,
                    0.6639204693817451,
                    0.6547519153558796,
                    0.6614108045088192,
                    0.6546953157658919,
                    0.6567892884303758,
                    0.6527341465714952,
                    0.6532964741718144,
                    0.6594629036536901,
                    0.6520799760171302,
                    0.6490572210396527,
                    0.6536844600655053,
                    0.656349019291745,
                    0.6475982316572901,
                    0.652164075142023,
                    0.6539490111983082,
                    0.6459744968064005,
                    0.6521581570246995,
                    0.6470790593366159,
                    0.6507464716361213,
                    0.6530312315087465,
                    0.6409335012086184,
                    0.6543139779365474,
                    0.6444904795453124,
                    0.6425286375091109,
                    0.6382238183566459,
                    0.6477521092710818,
                    0.6485662048780353,
                    0.639033040366876,
                    0.6478439084374645,
                    0.6483811908872577,
                    0.6431624679593667,
                    0.6386752511433084,
                    0.6402896328730253,
                    0.6457785371733917,
                    0.6369292352982963,
                    0.6386193958386569,
                    0.6429813784879851,
                    0.6386134366048131,
                    0.6406780908912153,
                    0.6396368873149405,
                    0.6373340856193275,
                    0.6390238117078432,
                    0.6309609394448973,
                    0.6452273301847002,
                    0.6359867521307685,
                    0.6356670997474013,
                    0.6321866000488534,
                    0.6391309183782561,
                    0.6312689977422566,
                    0.6320828796005432,
                    0.6337504614664482,
                    0.6293836112459704,
                    0.6424598850491104,
                    0.6278730400679359,
                    0.6365998930151118,
                    0.6270367528494357,
                    0.6294320295451545,
                    0.6448304632585615,
                    0.6191088019962042,
                    0.632385130418362,
                    0.6269391393110099,
                    0.6312347940346954,
                    0.6237600882002242,
                    0.6261988720128006,
                    0.6301185271384316,
                    0.6278985755782808,
                    0.627572917583352,
                    0.6358896075543994,
                    0.6161990579165203,
                    0.6315295229500181,
                    0.6231072864253145,
                    0.6268447127369698,
                    0.6225003184090045,
                    0.6217205818151046,
                    0.6276210819283994,
                    0.6291711819376057,
                    0.623773479364364,
                    0.6227031235658252,
                    0.6277829778694313,
                    0.6200388227988908,
                    0.628005826805221,
                    0.6260842755913887,
                    0.6188478250862618,
                    0.6229267372545833,
                    0.6243000693390296,
                    0.6193361472953754,
                    0.6212945171559132,
                    0.6155409326302257,
                    0.6203810227999079,
                    0.62268540448248,
                    0.6159850184241651,
                    0.6179939139057213,
                    0.6238059836359893,
                    0.6138792108486808,
                    0.6264927145356024,
                    0.6118572426294472,
                    0.6132781867763069,
                    0.6248317557540883,
                    0.6178889702313917,
                    0.6171397041772682,
                    0.6141583234281607,
                    0.617266090469919,
                    0.611621680527582,
                    0.6162315696363413,
                    0.6149567748764566,
                    0.6190535046599052,
                    0.6118470446210207,
                    0.614468187737671
                  ],
                  "yaxis": "y"
                },
                {
                  "line": {
                    "color": "green"
                  },
                  "name": "Validation Loss",
                  "type": "scatter",
                  "x": [
                    0,
                    1,
                    2,
                    3,
                    4,
                    5,
                    6,
                    7,
                    8,
                    9,
                    10,
                    11,
                    12,
                    13,
                    14,
                    15,
                    16,
                    17,
                    18,
                    19,
                    20,
                    21,
                    22,
                    23,
                    24,
                    25,
                    26,
                    27,
                    28,
                    29,
                    30,
                    31,
                    32,
                    33,
                    34,
                    35,
                    36,
                    37,
                    38,
                    39,
                    40,
                    41,
                    42,
                    43,
                    44,
                    45,
                    46,
                    47,
                    48,
                    49,
                    50,
                    51,
                    52,
                    53,
                    54,
                    55,
                    56,
                    57,
                    58,
                    59,
                    60,
                    61,
                    62,
                    63,
                    64,
                    65,
                    66,
                    67,
                    68,
                    69,
                    70,
                    71,
                    72,
                    73,
                    74,
                    75,
                    76,
                    77,
                    78,
                    79,
                    80,
                    81,
                    82,
                    83,
                    84,
                    85,
                    86,
                    87,
                    88,
                    89,
                    90,
                    91,
                    92,
                    93,
                    94,
                    95,
                    96,
                    97,
                    98,
                    99,
                    100,
                    101,
                    102,
                    103,
                    104,
                    105,
                    106,
                    107,
                    108,
                    109,
                    110,
                    111,
                    112,
                    113,
                    114,
                    115,
                    116,
                    117,
                    118,
                    119,
                    120,
                    121,
                    122,
                    123,
                    124,
                    125,
                    126,
                    127,
                    128,
                    129,
                    130,
                    131,
                    132,
                    133,
                    134,
                    135,
                    136,
                    137,
                    138,
                    139,
                    140,
                    141,
                    142,
                    143,
                    144,
                    145,
                    146,
                    147,
                    148,
                    149,
                    150,
                    151,
                    152,
                    153,
                    154,
                    155,
                    156,
                    157,
                    158,
                    159,
                    160,
                    161,
                    162,
                    163,
                    164,
                    165,
                    166,
                    167,
                    168,
                    169,
                    170,
                    171,
                    172,
                    173,
                    174,
                    175,
                    176,
                    177,
                    178,
                    179,
                    180,
                    181,
                    182,
                    183,
                    184,
                    185,
                    186,
                    187,
                    188,
                    189,
                    190,
                    191,
                    192,
                    193,
                    194,
                    195,
                    196,
                    197,
                    198,
                    199,
                    200,
                    201,
                    202,
                    203,
                    204,
                    205,
                    206,
                    207,
                    208,
                    209,
                    210,
                    211,
                    212,
                    213,
                    214,
                    215,
                    216,
                    217,
                    218,
                    219,
                    220,
                    221,
                    222,
                    223,
                    224,
                    225,
                    226,
                    227,
                    228,
                    229,
                    230,
                    231,
                    232,
                    233,
                    234,
                    235,
                    236,
                    237,
                    238,
                    239,
                    240,
                    241,
                    242,
                    243,
                    244,
                    245,
                    246,
                    247,
                    248,
                    249,
                    250,
                    251,
                    252,
                    253,
                    254,
                    255,
                    256,
                    257,
                    258,
                    259,
                    260,
                    261,
                    262,
                    263,
                    264,
                    265,
                    266,
                    267,
                    268,
                    269,
                    270,
                    271,
                    272,
                    273,
                    274,
                    275,
                    276,
                    277,
                    278,
                    279,
                    280,
                    281,
                    282,
                    283,
                    284,
                    285,
                    286,
                    287,
                    288,
                    289,
                    290,
                    291,
                    292,
                    293,
                    294,
                    295,
                    296,
                    297,
                    298,
                    299
                  ],
                  "xaxis": "x",
                  "y": [
                    2.3223689985580935,
                    2.3081502524706035,
                    2.3021531785145783,
                    2.2926894495120416,
                    2.305039957547799,
                    2.2899697698079624,
                    2.3077561389177275,
                    2.315843411745169,
                    2.3097303830660305,
                    2.31729732797696,
                    2.305281012486189,
                    2.336098175782424,
                    2.3395146826903024,
                    2.390246181915968,
                    2.366066818053906,
                    2.4053787619639664,
                    2.410941103330025,
                    2.4549994331139784,
                    2.465253538046128,
                    2.524981263356331,
                    2.541200113602174,
                    2.546500717218106,
                    2.5579349795977273,
                    2.602389673391978,
                    2.643250638857866,
                    2.664621656521773,
                    2.700241580223426,
                    2.759609858959149,
                    2.7572545554393377,
                    2.790826209844687,
                    2.833137963062678,
                    2.8626091931110773,
                    2.913117887882086,
                    2.9268758403949247,
                    2.9775330485441747,
                    3.0242533393395252,
                    3.0412578613330155,
                    3.048034533476218,
                    3.1286903734390554,
                    3.1697937700992975,
                    3.220017673877569,
                    3.2964639892944922,
                    3.279379276893078,
                    3.2989223163861494,
                    3.3500193640207634,
                    3.486097296843162,
                    3.431485224992801,
                    3.494798427208876,
                    3.529496725553121,
                    3.5983886879224043,
                    3.660318000958516,
                    3.745441589600001,
                    3.759047669477952,
                    3.7541909233117714,
                    3.8929200164782696,
                    3.866852958997091,
                    3.955282050065505,
                    4.034978033640446,
                    4.096499527112032,
                    4.169454375902812,
                    4.123909941086402,
                    4.235750037126052,
                    4.28243072445576,
                    4.321648282882495,
                    4.347155026900462,
                    4.383714510844304,
                    4.503178803584515,
                    4.499690451683143,
                    4.627106844614714,
                    4.554190455338894,
                    4.729653586943944,
                    4.720689325760572,
                    4.7453571527432175,
                    4.846385372754855,
                    4.870251365961173,
                    5.0171101337824116,
                    5.0200536617865925,
                    4.965353291768294,
                    5.059823135534923,
                    5.10464448424486,
                    5.15543210047942,
                    5.135887547181203,
                    5.250304607244638,
                    5.183643249365,
                    5.257710113739356,
                    5.400797115686612,
                    5.559823253215888,
                    5.418868793126864,
                    5.506199616652268,
                    5.476337331991929,
                    5.571504657085125,
                    5.62970984669832,
                    5.607642936400878,
                    5.658864930654183,
                    5.775349493210133,
                    5.775915786241874,
                    5.817910885199522,
                    5.818480092745561,
                    5.907771806686353,
                    5.845375865697861,
                    6.05259878665973,
                    5.865946562626423,
                    6.0969368402774515,
                    6.133262725976797,
                    6.003994579498585,
                    6.075191705654829,
                    6.188849287155347,
                    6.103896126533166,
                    6.348713887807651,
                    6.276162263674614,
                    6.32973961570324,
                    6.425998119207529,
                    6.268820516574077,
                    6.3651272142544775,
                    6.333312461773555,
                    6.462304705228561,
                    6.527719138524471,
                    6.560190190107394,
                    6.417364462063863,
                    6.63135841412422,
                    6.605537067621182,
                    6.588744009152437,
                    6.571453307683651,
                    6.605008592208226,
                    6.596012907150464,
                    6.8081157971651125,
                    6.7639699043371735,
                    6.71703639244422,
                    6.801312179137499,
                    6.781450904332674,
                    6.745900426155481,
                    6.8652213093562,
                    6.967794059178768,
                    6.938489136023399,
                    7.116062306440794,
                    7.0004023145406675,
                    6.961404221180158,
                    6.916156296546642,
                    7.050527833975279,
                    7.110554146460998,
                    7.281101679190611,
                    7.0928844549717045,
                    7.104252778566801,
                    6.950312240765645,
                    7.159550946492415,
                    7.304938997977819,
                    7.153588389739012,
                    7.337202527584174,
                    7.273969414906624,
                    7.333150489208026,
                    7.188023818609042,
                    7.4548547726411085,
                    7.364526780752035,
                    7.513754785060883,
                    7.471260128877102,
                    7.352606354615627,
                    7.577930271625519,
                    7.519692396506285,
                    7.422740811720873,
                    7.695952546902192,
                    7.701843934181409,
                    7.56989867412127,
                    7.654386622783465,
                    7.767080145004468,
                    7.61592343984506,
                    7.674739259939927,
                    7.582805032913502,
                    7.807735275763732,
                    7.6386761252696695,
                    7.705444111273839,
                    7.739956837434035,
                    7.989787604564276,
                    7.676230274713957,
                    7.9450653699728155,
                    7.958547231478569,
                    7.760643492906522,
                    8.092023788354336,
                    7.921635024058513,
                    8.17898366237298,
                    7.865346647225893,
                    8.052564897598364,
                    8.095771075823368,
                    8.075313861553486,
                    8.053583302558996,
                    7.8341483030563746,
                    7.98729991607177,
                    7.925249984631171,
                    8.122778134468275,
                    8.008446383170593,
                    8.036720726734552,
                    8.060755104590685,
                    7.99918055610779,
                    8.08870352537204,
                    8.242288728555044,
                    8.119619297675598,
                    8.293779117938799,
                    8.17067534648455,
                    8.316397381898684,
                    8.273826224681658,
                    8.370853929947584,
                    8.27264925149771,
                    8.164911297651438,
                    8.298115582038195,
                    8.416400586947416,
                    8.289295135400234,
                    8.343288354384594,
                    8.285720290281834,
                    8.555552216676565,
                    8.564595297361032,
                    8.437772772251032,
                    8.382888129124275,
                    8.635911808564114,
                    8.559944487535036,
                    8.625206416998154,
                    8.585760993835253,
                    8.65914656718572,
                    8.610070172028664,
                    8.477666364266323,
                    8.908282151589027,
                    8.739235434776697,
                    8.473067890375088,
                    8.600095913960384,
                    8.472866636056166,
                    8.76351838081311,
                    8.757421950499216,
                    8.753300870076204,
                    8.7779367963473,
                    8.80880830456049,
                    8.728752964582199,
                    8.734692150201553,
                    8.776779404053322,
                    8.565202680917887,
                    8.667667508125305,
                    8.836330468838032,
                    8.644345216262035,
                    8.676905165880154,
                    8.797804948611137,
                    8.751201828320822,
                    8.907087043309824,
                    8.734529770337618,
                    8.9017884181096,
                    8.822684080172808,
                    8.788531361482082,
                    8.79618890163226,
                    8.847159414719313,
                    8.802671366777176,
                    8.73129262832495,
                    9.128964546399239,
                    9.211522677005865,
                    8.897309937538244,
                    9.035847147305807,
                    9.202406623424627,
                    8.955375743217957,
                    9.082112102936476,
                    9.422531551275497,
                    9.040754093573643,
                    9.139663989727314,
                    9.190552855149294,
                    8.885963634038584,
                    9.120594382286072,
                    8.83820589689108,
                    9.008147516311743,
                    9.399410174443172,
                    9.13419718772937,
                    9.131805860079252,
                    9.254929554768097,
                    9.168210379588299,
                    9.25019343999716,
                    9.41279405202621,
                    9.38808406316317,
                    9.31684107811023,
                    9.314910346116775,
                    9.179785578678816,
                    9.45746810008318,
                    9.327082050152313,
                    9.331180022313045,
                    9.477327906168425,
                    9.279878457387289,
                    9.308079580465952,
                    9.326260404709059,
                    9.431367811484215,
                    9.422067321263826,
                    9.417780099770962,
                    9.483411527596987,
                    9.296585932756082,
                    9.417461971441904,
                    9.342581452467503,
                    9.493816705850454,
                    9.596191277870766,
                    9.501788364006924,
                    9.436898271242777,
                    9.541314633993002,
                    9.661966918370663,
                    9.526109446317722,
                    9.672206883247082,
                    9.478147514355488,
                    9.522224637178274,
                    9.66963584912129,
                    9.645353139975132,
                    9.797137515667158
                  ],
                  "yaxis": "y"
                }
              ],
              "layout": {
                "template": {
                  "data": {
                    "bar": [
                      {
                        "error_x": {
                          "color": "#2a3f5f"
                        },
                        "error_y": {
                          "color": "#2a3f5f"
                        },
                        "marker": {
                          "line": {
                            "color": "#E5ECF6",
                            "width": 0.5
                          },
                          "pattern": {
                            "fillmode": "overlay",
                            "size": 10,
                            "solidity": 0.2
                          }
                        },
                        "type": "bar"
                      }
                    ],
                    "barpolar": [
                      {
                        "marker": {
                          "line": {
                            "color": "#E5ECF6",
                            "width": 0.5
                          },
                          "pattern": {
                            "fillmode": "overlay",
                            "size": 10,
                            "solidity": 0.2
                          }
                        },
                        "type": "barpolar"
                      }
                    ],
                    "carpet": [
                      {
                        "aaxis": {
                          "endlinecolor": "#2a3f5f",
                          "gridcolor": "white",
                          "linecolor": "white",
                          "minorgridcolor": "white",
                          "startlinecolor": "#2a3f5f"
                        },
                        "baxis": {
                          "endlinecolor": "#2a3f5f",
                          "gridcolor": "white",
                          "linecolor": "white",
                          "minorgridcolor": "white",
                          "startlinecolor": "#2a3f5f"
                        },
                        "type": "carpet"
                      }
                    ],
                    "choropleth": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "type": "choropleth"
                      }
                    ],
                    "contour": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ],
                        "type": "contour"
                      }
                    ],
                    "contourcarpet": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "type": "contourcarpet"
                      }
                    ],
                    "heatmap": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ],
                        "type": "heatmap"
                      }
                    ],
                    "heatmapgl": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ],
                        "type": "heatmapgl"
                      }
                    ],
                    "histogram": [
                      {
                        "marker": {
                          "pattern": {
                            "fillmode": "overlay",
                            "size": 10,
                            "solidity": 0.2
                          }
                        },
                        "type": "histogram"
                      }
                    ],
                    "histogram2d": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ],
                        "type": "histogram2d"
                      }
                    ],
                    "histogram2dcontour": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ],
                        "type": "histogram2dcontour"
                      }
                    ],
                    "mesh3d": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "type": "mesh3d"
                      }
                    ],
                    "parcoords": [
                      {
                        "line": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "parcoords"
                      }
                    ],
                    "pie": [
                      {
                        "automargin": true,
                        "type": "pie"
                      }
                    ],
                    "scatter": [
                      {
                        "fillpattern": {
                          "fillmode": "overlay",
                          "size": 10,
                          "solidity": 0.2
                        },
                        "type": "scatter"
                      }
                    ],
                    "scatter3d": [
                      {
                        "line": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scatter3d"
                      }
                    ],
                    "scattercarpet": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattercarpet"
                      }
                    ],
                    "scattergeo": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattergeo"
                      }
                    ],
                    "scattergl": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattergl"
                      }
                    ],
                    "scattermapbox": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattermapbox"
                      }
                    ],
                    "scatterpolar": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scatterpolar"
                      }
                    ],
                    "scatterpolargl": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scatterpolargl"
                      }
                    ],
                    "scatterternary": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scatterternary"
                      }
                    ],
                    "surface": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ],
                        "type": "surface"
                      }
                    ],
                    "table": [
                      {
                        "cells": {
                          "fill": {
                            "color": "#EBF0F8"
                          },
                          "line": {
                            "color": "white"
                          }
                        },
                        "header": {
                          "fill": {
                            "color": "#C8D4E3"
                          },
                          "line": {
                            "color": "white"
                          }
                        },
                        "type": "table"
                      }
                    ]
                  },
                  "layout": {
                    "annotationdefaults": {
                      "arrowcolor": "#2a3f5f",
                      "arrowhead": 0,
                      "arrowwidth": 1
                    },
                    "autotypenumbers": "strict",
                    "coloraxis": {
                      "colorbar": {
                        "outlinewidth": 0,
                        "ticks": ""
                      }
                    },
                    "colorscale": {
                      "diverging": [
                        [
                          0,
                          "#8e0152"
                        ],
                        [
                          0.1,
                          "#c51b7d"
                        ],
                        [
                          0.2,
                          "#de77ae"
                        ],
                        [
                          0.3,
                          "#f1b6da"
                        ],
                        [
                          0.4,
                          "#fde0ef"
                        ],
                        [
                          0.5,
                          "#f7f7f7"
                        ],
                        [
                          0.6,
                          "#e6f5d0"
                        ],
                        [
                          0.7,
                          "#b8e186"
                        ],
                        [
                          0.8,
                          "#7fbc41"
                        ],
                        [
                          0.9,
                          "#4d9221"
                        ],
                        [
                          1,
                          "#276419"
                        ]
                      ],
                      "sequential": [
                        [
                          0,
                          "#0d0887"
                        ],
                        [
                          0.1111111111111111,
                          "#46039f"
                        ],
                        [
                          0.2222222222222222,
                          "#7201a8"
                        ],
                        [
                          0.3333333333333333,
                          "#9c179e"
                        ],
                        [
                          0.4444444444444444,
                          "#bd3786"
                        ],
                        [
                          0.5555555555555556,
                          "#d8576b"
                        ],
                        [
                          0.6666666666666666,
                          "#ed7953"
                        ],
                        [
                          0.7777777777777778,
                          "#fb9f3a"
                        ],
                        [
                          0.8888888888888888,
                          "#fdca26"
                        ],
                        [
                          1,
                          "#f0f921"
                        ]
                      ],
                      "sequentialminus": [
                        [
                          0,
                          "#0d0887"
                        ],
                        [
                          0.1111111111111111,
                          "#46039f"
                        ],
                        [
                          0.2222222222222222,
                          "#7201a8"
                        ],
                        [
                          0.3333333333333333,
                          "#9c179e"
                        ],
                        [
                          0.4444444444444444,
                          "#bd3786"
                        ],
                        [
                          0.5555555555555556,
                          "#d8576b"
                        ],
                        [
                          0.6666666666666666,
                          "#ed7953"
                        ],
                        [
                          0.7777777777777778,
                          "#fb9f3a"
                        ],
                        [
                          0.8888888888888888,
                          "#fdca26"
                        ],
                        [
                          1,
                          "#f0f921"
                        ]
                      ]
                    },
                    "colorway": [
                      "#636efa",
                      "#EF553B",
                      "#00cc96",
                      "#ab63fa",
                      "#FFA15A",
                      "#19d3f3",
                      "#FF6692",
                      "#B6E880",
                      "#FF97FF",
                      "#FECB52"
                    ],
                    "font": {
                      "color": "#2a3f5f"
                    },
                    "geo": {
                      "bgcolor": "white",
                      "lakecolor": "white",
                      "landcolor": "#E5ECF6",
                      "showlakes": true,
                      "showland": true,
                      "subunitcolor": "white"
                    },
                    "hoverlabel": {
                      "align": "left"
                    },
                    "hovermode": "closest",
                    "mapbox": {
                      "style": "light"
                    },
                    "paper_bgcolor": "white",
                    "plot_bgcolor": "#E5ECF6",
                    "polar": {
                      "angularaxis": {
                        "gridcolor": "white",
                        "linecolor": "white",
                        "ticks": ""
                      },
                      "bgcolor": "#E5ECF6",
                      "radialaxis": {
                        "gridcolor": "white",
                        "linecolor": "white",
                        "ticks": ""
                      }
                    },
                    "scene": {
                      "xaxis": {
                        "backgroundcolor": "#E5ECF6",
                        "gridcolor": "white",
                        "gridwidth": 2,
                        "linecolor": "white",
                        "showbackground": true,
                        "ticks": "",
                        "zerolinecolor": "white"
                      },
                      "yaxis": {
                        "backgroundcolor": "#E5ECF6",
                        "gridcolor": "white",
                        "gridwidth": 2,
                        "linecolor": "white",
                        "showbackground": true,
                        "ticks": "",
                        "zerolinecolor": "white"
                      },
                      "zaxis": {
                        "backgroundcolor": "#E5ECF6",
                        "gridcolor": "white",
                        "gridwidth": 2,
                        "linecolor": "white",
                        "showbackground": true,
                        "ticks": "",
                        "zerolinecolor": "white"
                      }
                    },
                    "shapedefaults": {
                      "line": {
                        "color": "#2a3f5f"
                      }
                    },
                    "ternary": {
                      "aaxis": {
                        "gridcolor": "white",
                        "linecolor": "white",
                        "ticks": ""
                      },
                      "baxis": {
                        "gridcolor": "white",
                        "linecolor": "white",
                        "ticks": ""
                      },
                      "bgcolor": "#E5ECF6",
                      "caxis": {
                        "gridcolor": "white",
                        "linecolor": "white",
                        "ticks": ""
                      }
                    },
                    "title": {
                      "x": 0.05
                    },
                    "xaxis": {
                      "automargin": true,
                      "gridcolor": "white",
                      "linecolor": "white",
                      "ticks": "",
                      "title": {
                        "standoff": 15
                      },
                      "zerolinecolor": "white",
                      "zerolinewidth": 2
                    },
                    "yaxis": {
                      "automargin": true,
                      "gridcolor": "white",
                      "linecolor": "white",
                      "ticks": "",
                      "title": {
                        "standoff": 15
                      },
                      "zerolinecolor": "white",
                      "zerolinewidth": 2
                    }
                  }
                },
                "xaxis": {
                  "anchor": "y",
                  "domain": [
                    0,
                    0.26666666666666666
                  ],
                  "title": {
                    "text": "Epochs"
                  }
                },
                "xaxis2": {
                  "anchor": "y2",
                  "domain": [
                    0.3666666666666667,
                    0.6333333333333333
                  ]
                },
                "xaxis3": {
                  "anchor": "y3",
                  "domain": [
                    0.7333333333333334,
                    1
                  ]
                },
                "yaxis": {
                  "anchor": "x",
                  "domain": [
                    0,
                    1
                  ],
                  "title": {
                    "text": "Training Loss"
                  }
                },
                "yaxis2": {
                  "anchor": "x2",
                  "domain": [
                    0,
                    1
                  ]
                },
                "yaxis3": {
                  "anchor": "x3",
                  "domain": [
                    0,
                    1
                  ]
                }
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig = make_subplots(rows=1, cols=3, shared_yaxes=False, shared_xaxes=True, horizontal_spacing=0.1, vertical_spacing=0.1)\n",
        "\n",
        "for idx in range(len(noise_training_loss_arr)):\n",
        "    fig.add_scatter(\n",
        "        x=np.arange(TRAINING_RUNS), y=noise_training_loss_arr[idx],\n",
        "        col=idx+1, row=1, name = \"Training Loss\", line=dict(color='red'),\n",
        "    )\n",
        "    fig.add_scatter(\n",
        "        x=np.arange(TRAINING_RUNS), y=noise_validation_loss_arr[idx],\n",
        "        col=idx+1, row=1, name = \"Validation Loss\", line=dict(color='green')\n",
        "    )\n",
        "    fig.update_xaxes(title_text='Epochs', row=1, col=idx+1)\n",
        "    fig.update_yaxes(title_text='Training Loss', row=1, col=idx+1)\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLkkTsGQrYZq"
      },
      "source": [
        "One training run is enough to get good test score accuracy, however training loss is still high. I guess it knows the right answer after a single epoch but it is still unsure?"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "interpreter": {
      "hash": "f59ef49e738fb3c4415b7718da7995bff3ea929ace96d22cb5fa9ff2453b09e7"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('Dissertation')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "ff87dbf42c9199796f315001a4590a6a5b463d121b2767c3ff75ede187152b04"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
